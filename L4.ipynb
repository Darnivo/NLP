{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import string\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from random import shuffle\n",
    "\n",
    "from nltk import FreqDist\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import PorterStemmer,WordNetLemmatizer\n",
    "from nltk.tag import pos_tag\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "import spacy\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "wnl = WordNetLemmatizer()\n",
    "stopw = stopwords.words(\"english\")\n",
    "\n",
    "dataset = pd.read_csv(\"./updated_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preproc(document):\n",
    "    words = word_tokenize(document.lower())\n",
    "    \n",
    "    words = [wnl.lemmatize(word) for word in words]\n",
    "    words = [stemmer.stem(word) for word in words]\n",
    "    \n",
    "    return {word:True for word in words if word not in stopw and word.isalpha()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainmodel():\n",
    "    fset = [(preproc(txt),label) for txt,label in zip(dataset[\"text\"],dataset[\"label\"])]\n",
    "    shuffle(fset)\n",
    "    \n",
    "    split_idx = int(len(fset)*0.85)\n",
    "    trains,tests = fset[:split_idx],fset[split_idx:]\n",
    "    \n",
    "    clf = nltk.NaiveBayesClassifier.train(trains)\n",
    "    \n",
    "    acc = nltk.classify.accuracy(clf,tests)\n",
    "    print(f\"acc > {acc}\")\n",
    "    \n",
    "    clf.show_most_informative_features(5)\n",
    "    \n",
    "    \n",
    "    file = open(\"model.pickle\",\"wb\")\n",
    "    pickle.dump(clf,file)\n",
    "    file.close()\n",
    "    \n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readmodel():\n",
    "    try:\n",
    "        file = open(\"model.pickle\",\"rb\")\n",
    "        clf = pickle.load(file)\n",
    "        clf.show_most_informative_features(5)\n",
    "        file.close()\n",
    "    except:\n",
    "        clf = trainmodel()\n",
    "        \n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def writerev():\n",
    "    while True:\n",
    "        review = input(\"Input must be >=2 words\")\n",
    "        \n",
    "        if len(review.split())>1:\n",
    "            return review\n",
    "        else:\n",
    "            print(\"Too short\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "                 terribl = True           negati : positi =     13.8 : 1.0\n",
      "                 perfect = True           positi : negati =     13.5 : 1.0\n",
      "                 horribl = True           negati : positi =      9.8 : 1.0\n",
      "                   anyon = True           negati : positi =      9.1 : 1.0\n",
      "                  receiv = True           negati : positi =      8.4 : 1.0\n"
     ]
    }
   ],
   "source": [
    "classifier = readmodel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze(review:str,clf):\n",
    "    words = word_tokenize(review.lower())\n",
    "    words = [word for word in words if word not in string.punctuation and word.isalpha()]\n",
    "    \n",
    "    tags = pos_tag(words)\n",
    "    for i,word in enumerate(tags):\n",
    "        print(f\"{i+1}. {word[0]} {word[1]}\")\n",
    "    print(\"\")    \n",
    "    \n",
    "    for word in words:\n",
    "        print(\"word > \",word)\n",
    "        synos = []\n",
    "        antos = []\n",
    "        \n",
    "        synset = wordnet.synsets(word)\n",
    "        \n",
    "        for syns in synset:\n",
    "            for lemm in syns.lemmas():\n",
    "                synos.append(lemm.name())\n",
    "                for anto in lemm.antonyms():\n",
    "                    antos.append(anto.name())\n",
    "        \n",
    "        \n",
    "        #use to remove dupes         \n",
    "        synos = list(set(synos))\n",
    "        antos = list(set(antos))\n",
    "                    \n",
    "        print(\", \".join(synos) if synos else \"no syns\")\n",
    "        print(\", \".join(antos) if antos else \"no ants\")\n",
    "        print(\"===\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    print(\"\")\n",
    "    \n",
    "    cleaned_rev = [word for word in word_tokenize(review) if word not in string.punctuation and word not in stopw]\n",
    "    cleaned_rev = [wnl.lemmatize(stemmer.stem(word)) for word in cleaned_rev]\n",
    "    \n",
    "    result = clf.classify(FreqDist(cleaned_rev))\n",
    "    print(f\"{review} > {result}\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "# analyze(\"owen loves little children\", classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17 > Restaurant C 0.4629912053109412\n",
      "387 > Restaurant C 0.16381587462932606\n",
      "195 > Restaurant A 0.15969550308016198\n",
      "170 > Restaurant A 0.15228221911813097\n",
      "305 > Restaurant A 0.15161707817897763\n"
     ]
    }
   ],
   "source": [
    "def recomm(review):\n",
    "    corpus = dataset[\"text\"]\n",
    "    target = dataset[\"restaurant\"]\n",
    "    \n",
    "    tfidf = TfidfVectorizer()\n",
    "    tfidf_mtr = tfidf.fit_transform(corpus)\n",
    "    query = tfidf.transform([review])\n",
    "    \n",
    "    simil = cosine_similarity(tfidf_mtr,query)\n",
    "    \n",
    "    df = pd.DataFrame({\n",
    "        'doc':corpus,\n",
    "        'target':target,\n",
    "        'simil':simil.flatten()\n",
    "    })\n",
    "    \n",
    "    recomms = df[df[\"simil\"]>0.15]\n",
    "    \n",
    "    top_recomms = recomms.sort_values(by='simil',ascending=False).head(10)\n",
    "    \n",
    "    # top_recomms = top_recomms.reset_index(drop=True)\n",
    "    \n",
    "    return top_recomms[['target','simil']]\n",
    "\n",
    "\n",
    "recommends = recomm(\"My orders have come out bad pretty much every time I have ordered from here.\")\n",
    "\n",
    "if recommends.empty:\n",
    "    print(\"no recomms\")\n",
    "else:\n",
    "    for idx, row in recommends.iterrows():\n",
    "        print(f\"{idx} > {row['target']} {row['simil']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPE > Indonesia\n",
      "ORG > Apple inc, microsoft\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "ner_label = nlp.get_pipe(\"ner\").labels\n",
    "\n",
    "# print(ner_label)\n",
    "\n",
    "def ner(review):\n",
    "    categorized_data = defaultdict(set)\n",
    "    doc = nlp(review)\n",
    "    \n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ in [\"GPE\",\"ORG\",\"LANGUAGE\"]:\n",
    "            categorized_data[ent.label_].add(ent.text)\n",
    "            \n",
    "    for cat,ent in categorized_data.items():\n",
    "        print(f\"{cat} > {\", \".join(sorted(ent))}\")\n",
    "        \n",
    "ner(\"Indonesia seeks to ban microsoft in favor of Apple inc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "                 terribl = True           negati : positi =     13.8 : 1.0\n",
      "                 perfect = True           positi : negati =     13.5 : 1.0\n",
      "                 horribl = True           negati : positi =      9.8 : 1.0\n",
      "                   anyon = True           negati : positi =      9.1 : 1.0\n",
      "                  receiv = True           negati : positi =      8.4 : 1.0\n",
      "Menu, select command\n",
      "rev > no review\n",
      "sent > no sent\n",
      ">>2\n",
      "no rev!\n",
      "\n",
      "Menu, select command\n",
      "rev > no review\n",
      "sent > no sent\n",
      ">>1\n",
      "\n",
      "Menu, select command\n",
      "rev > Overpriced, salty and overrated food from Chile only rivaled by microsoft cafetaria food\n",
      "sent > no sent\n",
      ">>2\n",
      "1. overpriced VBN\n",
      "2. salty NN\n",
      "3. and CC\n",
      "4. overrated VBD\n",
      "5. food NN\n",
      "6. from IN\n",
      "7. chile NN\n",
      "8. only RB\n",
      "9. rivaled VBN\n",
      "10. by IN\n",
      "11. microsoft JJ\n",
      "12. cafetaria NNS\n",
      "13. food NN\n",
      "\n",
      "word >  overpriced\n",
      "overprice, overpriced\n",
      "no ants\n",
      "===\n",
      "word >  salty\n",
      "piquant, salty\n",
      "fresh\n",
      "===\n",
      "word >  and\n",
      "no syns\n",
      "no ants\n",
      "===\n",
      "word >  overrated\n",
      "overrate, overestimate\n",
      "underestimate\n",
      "===\n",
      "word >  food\n",
      "nutrient, food, food_for_thought, intellectual_nourishment, solid_food\n",
      "no ants\n",
      "===\n",
      "word >  from\n",
      "no syns\n",
      "no ants\n",
      "===\n",
      "word >  chile\n",
      "chilli, chili_pepper, Republic_of_Chile, Chile, chilly, chile, chili\n",
      "no ants\n",
      "===\n",
      "word >  only\n",
      "solely, solitary, alone, sole, merely, only, simply, but, lone, just, exclusively, only_when, lonesome, entirely, only_if\n",
      "no ants\n",
      "===\n",
      "word >  rivaled\n",
      "equal, touch, match, rival\n",
      "no ants\n",
      "===\n",
      "word >  by\n",
      "by, past, away, aside\n",
      "no ants\n",
      "===\n",
      "word >  microsoft\n",
      "no syns\n",
      "no ants\n",
      "===\n",
      "word >  cafetaria\n",
      "no syns\n",
      "no ants\n",
      "===\n",
      "word >  food\n",
      "nutrient, food, food_for_thought, intellectual_nourishment, solid_food\n",
      "no ants\n",
      "===\n",
      "\n",
      "Overpriced, salty and overrated food from Chile only rivaled by microsoft cafetaria food > negative\n",
      "\n",
      "Menu, select command\n",
      "rev > Overpriced, salty and overrated food from Chile only rivaled by microsoft cafetaria food\n",
      "sent > negative\n",
      ">>3\n",
      "38 > Restaurant D 0.5143559201077189\n",
      "227 > Restaurant C 0.1594922229081678\n",
      "\n",
      "Menu, select command\n",
      "rev > Overpriced, salty and overrated food from Chile only rivaled by microsoft cafetaria food\n",
      "sent > negative\n",
      ">>4\n",
      "GPE > Chile\n",
      "ORG > microsoft cafetaria food\n",
      "\n",
      "Menu, select command\n",
      "rev > Overpriced, salty and overrated food from Chile only rivaled by microsoft cafetaria food\n",
      "sent > negative\n",
      ">>6\n",
      "input out of range, try again\n",
      "\n",
      "Menu, select command\n",
      "rev > Overpriced, salty and overrated food from Chile only rivaled by microsoft cafetaria food\n",
      "sent > negative\n",
      ">>5\n"
     ]
    }
   ],
   "source": [
    "classifier = readmodel()\n",
    "review =\"\"\n",
    "sent = \"\"\n",
    "\n",
    "while True:\n",
    "    print(\"Menu, select command\")\n",
    "    print(f\"rev > {review if review else \"no review\"}\")\n",
    "    print(f\"sent > {sent if sent else \"no sent\"}\")\n",
    "    \n",
    "    print(\">>\",flush=True, end = \"\")\n",
    "    choice = input(\"enter 1-5\")\n",
    "    print(choice)\n",
    "    \n",
    "    \n",
    "    if choice == '1':\n",
    "        review = writerev()\n",
    "    elif choice == '2':\n",
    "        if review:\n",
    "            sent = analyze(review, classifier)\n",
    "        else:\n",
    "            print(\"no rev!\")\n",
    "    elif choice == '3':\n",
    "        recommends = recomm(review)\n",
    "        if recommends.empty:\n",
    "            print(\"no recomms\")\n",
    "        else:\n",
    "            for idx, row in recommends.iterrows():\n",
    "                print(f\"{idx} > {row['target']} {row['simil']}\")\n",
    "    elif choice == '4':\n",
    "        ner(review)\n",
    "    elif choice == '5':\n",
    "        break\n",
    "    else:\n",
    "        print(\"input out of range, try again\")\n",
    "        \n",
    "    print(\"\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
