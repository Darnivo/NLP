{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 most informative features:\n",
      "Most Informative Features\n",
      "                    love = True           positi : negati =     13.3 : 1.0\n",
      "                   delay = True           negati : positi =     12.7 : 1.0\n",
      "                   thank = True           positi : negati =      9.9 : 1.0\n",
      "                    amaz = True           positi : negati =      8.8 : 1.0\n",
      "                  number = True           negati : positi =      8.6 : 1.0\n",
      "Accuracy:  0.8571428571428571\n"
     ]
    }
   ],
   "source": [
    "def trainModel():\n",
    "    # Read csv\n",
    "    import pandas as pd\n",
    "    \n",
    "    dataset = pd.read_csv(\"./dataset.csv\")\n",
    "    \n",
    "    feature_set = [(cleanText(text),label) for text,label in zip(dataset[\"tweet\"],dataset[\"sentiment\"])]\n",
    "    \n",
    "    from random import shuffle\n",
    "    \n",
    "    shuffle(feature_set)\n",
    "    \n",
    "    split_idx = int(len(feature_set)*0.85)\n",
    "    train_set = feature_set[:split_idx]\n",
    "    test_set = feature_set[split_idx:]\n",
    "    \n",
    "    # make model w/ naive bayes\n",
    "    from nltk.classify import NaiveBayesClassifier\n",
    "    classifier = NaiveBayesClassifier.train(train_set)\n",
    "    \n",
    "    # Show 5 most informative features and training accuracy\n",
    "    print(\"5 most informative features:\")\n",
    "    classifier.show_most_informative_features(5)\n",
    "    \n",
    "    \n",
    "    from nltk.classify import accuracy\n",
    "    accu = accuracy(classifier,test_set)\n",
    "    print(\"Accuracy: \", accu)\n",
    "    \n",
    "    #save model\n",
    "    import pickle\n",
    "    file = open(\"model.pickle\",\"wb\")\n",
    "    pickle.dump(classifier,file)\n",
    "    file.close()\n",
    "    \n",
    "    \n",
    "def cleanText(text): # preprocessing\n",
    "    \n",
    "    #tokenize\n",
    "    from nltk.tokenize import word_tokenize\n",
    "    txt = word_tokenize(text.lower())\n",
    "    \n",
    "    #remove stopwords\n",
    "    from nltk.corpus import stopwords\n",
    "    stopWords_en = stopwords.words(\"english\")\n",
    "    txt = [word for word in txt if not word in stopWords_en]\n",
    "    \n",
    "    \n",
    "    #remove symbols &num\n",
    "    txt = [word for word in txt if word.isalpha()]\n",
    "    \n",
    "    \n",
    "    #stem & lemmatize\n",
    "    from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "    stemmer = PorterStemmer()\n",
    "    lemmer = WordNetLemmatizer()\n",
    "    \n",
    "    txt = [stemmer.stem(word) for word in txt]      # stem\n",
    "    txt = [lemmer.lemmatize(word) for word in txt]  # lemmatize\n",
    "    \n",
    "    \n",
    "    # remove punctuation (?)\n",
    "    from string import punctuation\n",
    "    txt = [word for word in txt if not word in punctuation]\n",
    "    \n",
    "    # return a dict\n",
    "    return {word: True for word in txt}\n",
    "\n",
    "\n",
    "# print(cleanText(\"I love to hide in the bushes at night, but in 2001 i got caught by my neighbors\"))\n",
    "\n",
    "trainModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, the application will check whether there is “model.pickle” file in the application\n",
    "# directory or not.\n",
    "def readModel():\n",
    "    try:\n",
    "        file = open(\"model.pickle\",\"rb\")\n",
    "        print(\"Model exists!\")\n",
    "        # If the file exists, then the application will read and load the data training from the file.\n",
    "        \n",
    "        print(\"Loading model..\")\n",
    "        import pickle\n",
    "        classifier = pickle.load(file)\n",
    "        file.close()\n",
    "        \n",
    "        print(\"Model info:\",end = \"\")\n",
    "        classifier.show_most_informative_features(5)\n",
    "        \n",
    "        \n",
    "    except:\n",
    "        print(\"no model, making new one!\")\n",
    "        # train new model with naive bayes using dataset.csv\n",
    "        \n",
    "        classifier = trainModel()\n",
    "\n",
    "        \n",
    "        print(\"Training complete\")\n",
    "        \n",
    "    return classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "invalid\n",
      "['a', 'a', 'a', 'a', 'a']\n",
      "review added\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'a a a a a '"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def enterRev():\n",
    "    while True:\n",
    "        txt = input(\"Enter sentence (>=5 words)\")\n",
    "    \n",
    "        words = txt.split()\n",
    "        print(words)\n",
    "\n",
    "        if len(words) >4:\n",
    "            print(\"review added\")\n",
    "            return txt\n",
    "        else:\n",
    "            print(\"invalid\")\n",
    "    \n",
    "enterRev()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyzeRev(review,classifier):\n",
    "    # Check whether there is a tweet or not\n",
    "    import pandas as pd\n",
    "    \n",
    "    dataset = pd.read_csv(\"./dataset.csv\")\n",
    "    \n",
    "    # check if rev in dataset\n",
    "    if review in dataset[\"tweet\"].values:\n",
    "        print(\"Review is found.\")\n",
    "        \n",
    "        #semi clean text\n",
    "        from nltk.tokenize import word_tokenize\n",
    "        from nltk.probability import  FreqDist\n",
    "        from string import punctuation\n",
    "        \n",
    "        words = word_tokenize(review.lower())\n",
    "        words = FreqDist([word for word in words if word.isalpha() and word not in punctuation])\n",
    "        \n",
    "        \n",
    "        # do POS tagging\n",
    "        from nltk.tag import pos_tag\n",
    "        \n",
    "        tags = pos_tag(words)\n",
    "        for i, word in enumerate(tags):\n",
    "            print(f\"{i+1}. {word[0]} : {word[1]}\")\n",
    "        \n",
    "        \n",
    "        # show synonym & antonym\n",
    "        from nltk.corpus import wordnet\n",
    "        \n",
    "        for word in words:\n",
    "            print(\"=\"* 10)\n",
    "            print(f\"{word} =\")\n",
    "            print(\"=\"* 10)\n",
    "            \n",
    "            # synsets\n",
    "            Synsets = wordnet.synsets(word)\n",
    "            Syn = []\n",
    "            Ant = []\n",
    "            \n",
    "            for synset in Synsets:\n",
    "                for lemma in synset.lemmas():\n",
    "                    Syn.append(lemma.name())\n",
    "                    for anto in lemma.antonyms():\n",
    "                        Ant.append(anto.name())\n",
    "            \n",
    "            print(\"Synonyms: \")\n",
    "            if len(Syn) == 0:\n",
    "                print(\"no synos\")\n",
    "            else:\n",
    "                for synos in Syn[:5]:\n",
    "                    print(f\"(+){synos}\",end=\"  \")\n",
    "                print(\"\")\n",
    "            \n",
    "            \n",
    "            print(\"Antonyms: \")\n",
    "            if len(Ant) == 0:\n",
    "                print(\"no Anto\")\n",
    "            else:\n",
    "                for antos in Ant[:5]:\n",
    "                    print(f\"(-){antos}\",end=\"  \")\n",
    "                print(\"\")\n",
    "            \n",
    "            \n",
    "        # Predict and show the result of the tweet category.\n",
    "        from nltk.corpus import stopwords\n",
    "        stopWords_en = stopwords.words(\"english\")\n",
    "        clean_rev = [word for word in word_tokenize(review) if word not in punctuation and word not in stopWords_en]\n",
    "        result = classifier.classify(FreqDist(clean_rev))\n",
    "        \n",
    "        print(f\"Your Review: {review}\")\n",
    "        print(f\"Review Category: {result}\")\n",
    "        \n",
    "    else:\n",
    "        print(\"Review is not in the dataset, going back...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model exists!\n",
      "Loading model..\n",
      "Model info:Most Informative Features\n",
      "                    love = True           positi : negati =     14.6 : 1.0\n",
      "                    call = True           negati : positi =     14.0 : 1.0\n",
      "                    amaz = True           positi : negati =     10.5 : 1.0\n",
      "                   thank = True           positi : negati =     10.0 : 1.0\n",
      "                    site = True           negati : positi =      7.5 : 1.0\n",
      "===Title===\n",
      "Curr rev = No reviews\n",
      "1. write  rev\n",
      "2. analyze rev\n",
      "3. exit\n",
      "['@VirginAmerica', 'your', 'website', 'sucks', 'donkey', 'dicks.', 'Just', 'thought', 'you', 'should', 'know.', 'All', 'best.']\n",
      "review added\n",
      "===Title===\n",
      "Curr rev = @VirginAmerica your website sucks donkey dicks. Just thought you should know. All best.\n",
      "1. write  rev\n",
      "2. analyze rev\n",
      "3. exit\n",
      "Review is found.\n",
      "1. virginamerica : VB\n",
      "2. your : PRP$\n",
      "3. website : NN\n",
      "4. sucks : NNS\n",
      "5. donkey : JJ\n",
      "6. dicks : NNS\n",
      "7. just : RB\n",
      "8. thought : VBN\n",
      "9. you : PRP\n",
      "10. should : MD\n",
      "11. know : VB\n",
      "12. all : DT\n",
      "13. best : JJS\n",
      "==========\n",
      "virginamerica =\n",
      "==========\n",
      "Synonyms: \n",
      "no synos\n",
      "\n",
      "Antonyms: \n",
      "no Anto\n",
      "\n",
      "==========\n",
      "your =\n",
      "==========\n",
      "Synonyms: \n",
      "no synos\n",
      "\n",
      "Antonyms: \n",
      "no Anto\n",
      "\n",
      "==========\n",
      "website =\n",
      "==========\n",
      "Synonyms: \n",
      "(+)web_site  (+)website  (+)internet_site  (+)site  \n",
      "Antonyms: \n",
      "no Anto\n",
      "\n",
      "==========\n",
      "sucks =\n",
      "==========\n",
      "Synonyms: \n",
      "(+)sucking  (+)suck  (+)suction  (+)suck  (+)suck  \n",
      "Antonyms: \n",
      "(-)bottlefeed  \n",
      "==========\n",
      "donkey =\n",
      "==========\n",
      "Synonyms: \n",
      "(+)donkey  (+)domestic_ass  (+)donkey  (+)Equus_asinus  \n",
      "Antonyms: \n",
      "no Anto\n",
      "\n",
      "==========\n",
      "dicks =\n",
      "==========\n",
      "Synonyms: \n",
      "(+)dick  (+)gumshoe  (+)hawkshaw  (+)cock  (+)prick  \n",
      "Antonyms: \n",
      "no Anto\n",
      "\n",
      "==========\n",
      "just =\n",
      "==========\n",
      "Synonyms: \n",
      "(+)just  (+)equitable  (+)just  (+)fair  (+)just  \n",
      "Antonyms: \n",
      "(-)unjust  (-)inequitable  (-)unfair  \n",
      "==========\n",
      "thought =\n",
      "==========\n",
      "Synonyms: \n",
      "(+)idea  (+)thought  (+)thinking  (+)thought  (+)thought_process  \n",
      "Antonyms: \n",
      "(-)forget  \n",
      "==========\n",
      "you =\n",
      "==========\n",
      "Synonyms: \n",
      "no synos\n",
      "\n",
      "Antonyms: \n",
      "no Anto\n",
      "\n",
      "==========\n",
      "should =\n",
      "==========\n",
      "Synonyms: \n",
      "no synos\n",
      "\n",
      "Antonyms: \n",
      "no Anto\n",
      "\n",
      "==========\n",
      "know =\n",
      "==========\n",
      "Synonyms: \n",
      "(+)know  (+)know  (+)cognize  (+)cognise  (+)know  \n",
      "Antonyms: \n",
      "(-)ignore  \n",
      "==========\n",
      "all =\n",
      "==========\n",
      "Synonyms: \n",
      "(+)all  (+)all  (+)wholly  (+)entirely  (+)completely  \n",
      "Antonyms: \n",
      "(-)some  (-)no  (-)partly  \n",
      "==========\n",
      "best =\n",
      "==========\n",
      "Synonyms: \n",
      "(+)best  (+)best  (+)topper  (+)Best  (+)C._H._Best  \n",
      "Antonyms: \n",
      "(-)worst  (-)worst  (-)bad  (-)evil  (-)ill  \n",
      "Your Review: @VirginAmerica your website sucks donkey dicks. Just thought you should know. All best.\n",
      "Review Category: positive\n",
      "===Title===\n",
      "Curr rev = @VirginAmerica your website sucks donkey dicks. Just thought you should know. All best.\n",
      "1. write  rev\n",
      "2. analyze rev\n",
      "3. exit\n",
      "exiting..\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    classifier = readModel()\n",
    "    \n",
    "    review = \"\"\n",
    "    while True:\n",
    "        print(\"===Title===\")\n",
    "        print(\"Curr rev =\", review if review else \"No reviews\")\n",
    "        print(\"1. write  rev\")\n",
    "        print(\"2. analyze rev\")\n",
    "        print(\"3. exit\")\n",
    "        \n",
    "        choice = int(input(\"Enter input [1-3]\"))\n",
    "        if choice == 1:\n",
    "            review = enterRev()\n",
    "        elif choice == 2:\n",
    "            analyzeRev(review,classifier)\n",
    "        elif choice == 3:\n",
    "            print(\"exiting..\")\n",
    "            break\n",
    "        else:\n",
    "            print(\"wrong input\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
