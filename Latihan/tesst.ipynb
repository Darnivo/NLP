{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk, pandas as pd, pickle\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.stem import PorterStemmer,WordNetLemmatizer\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.classify import NaiveBayesClassifier,accuracy\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from string import punctuation\n",
    "from random import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "stems = PorterStemmer()\n",
    "lemms = WordNetLemmatizer()\n",
    "stopW = stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preproc(words):\n",
    "    tokens = word_tokenize(words)\n",
    "    words = [word.lower() for word in tokens if word.isalpha()]\n",
    "    words = [word for word in words if word not in stopW]\n",
    "    words = [stems.stem(word) for word in words]\n",
    "    words = [lemms.lemmatize(word) for word in words]\n",
    "    \n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = pd.read_csv(\"./dataset.csv\",encoding= \"utf-8-sig\")\n",
    "\n",
    "all_w = []\n",
    "for sent in ds[\"tweet\"]:\n",
    "    for word in preproc(sent):\n",
    "        all_w.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "fd = FreqDist(all_w)\n",
    "commonW = {word for  word in fd.most_common(1000)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_set = []\n",
    "\n",
    "for lbl,cat in zip(ds[\"tweet\"],ds[\"sentiment\"]):\n",
    "    feature = {}\n",
    "    \n",
    "    procd = preproc(lbl)\n",
    "    \n",
    "    for word in procd:\n",
    "        feature[word] = (word in commonW)\n",
    "        \n",
    "    feature_set.append((feature,cat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffle(feature_set)\n",
    "\n",
    "split_idx = int(len(feature_set)*0.7)\n",
    "trains = feature_set[:split_idx]\n",
    "tests = feature_set[split_idx:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "                    hour = False          negati : positi =     11.5 : 1.0\n",
      "                    love = False          positi : negati =     11.2 : 1.0\n",
      "                   thank = False          positi : negati =     10.1 : 1.0\n",
      "                  luggag = False          negati : positi =      8.7 : 1.0\n",
      "                   still = False          negati : positi =      8.7 : 1.0\n"
     ]
    }
   ],
   "source": [
    "clf = NaiveBayesClassifier.train(trains)\n",
    "clf.show_most_informative_features(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8380952380952381\n"
     ]
    }
   ],
   "source": [
    "print(accuracy(clf,tests))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"./modeltesst.pickle\",\"wb\")\n",
    "pickle.dump(clf,file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def synoanto(words):\n",
    "    SYN = []\n",
    "    ANT = []\n",
    "    \n",
    "    for syns in wordnet.synsets(words):\n",
    "        for lemmas in syns.lemmas():\n",
    "            SYN.append(lemmas.name())\n",
    "            for ants in lemmas.antonyms():\n",
    "                ANT.append(ants.name())\n",
    "\n",
    "    return SYN,ANT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "you\n",
      "-\n",
      "-\n",
      "==========\n",
      "are\n",
      "are, ar, be, be, be, exist, be, be, equal, be, constitute, represent, make_up, comprise, be, be, follow, embody, be, personify, be, be, live, be, cost, be\n",
      "differ\n",
      "==========\n",
      "late\n",
      "late, belated, late, tardy, late, recent, late, late, late, later, former, late, previous, late, belatedly, tardily, deep, late, late, recently, late, lately, of_late, latterly\n",
      "early, middle, early, middle, early, early\n",
      "==========\n"
     ]
    }
   ],
   "source": [
    "rev = \"you are late\"\n",
    "\n",
    "tok_rev = word_tokenize(rev)\n",
    "\n",
    "for word in tok_rev:\n",
    "    syn,ant = synoanto(word)\n",
    "    \n",
    "    print(word)\n",
    "    print(f\"{\", \".join(syn) if syn else \"-\"}\")\n",
    "    print(f\"{\", \".join(ant) if ant else \"-\"}\")\n",
    "    print(\"=\"*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tagger(word):\n",
    "    tokens = word_tokenize(word)\n",
    "    tags = pos_tag(tokens)\n",
    "    return tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. I --> PRP\n",
      "2. will --> MD\n",
      "3. come --> VB\n",
      "4. in --> IN\n",
      "5. the --> DT\n",
      "6. evening --> NN\n"
     ]
    }
   ],
   "source": [
    "rev = \"I will come in the evening\"\n",
    "\n",
    "tag_rev = tagger(rev)\n",
    "\n",
    "for i,word in enumerate(tag_rev):\n",
    "    print(f\"{i+1}. {word[0]} --> {word[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "negative\n"
     ]
    }
   ],
   "source": [
    "def analyze(words):\n",
    "    file = open(\"./modeltesst.pickle\",\"rb\")\n",
    "    classer = pickle.load(file)\n",
    "    file.close()\n",
    "    \n",
    "    clean = preproc(words)\n",
    "    \n",
    "    res = classer.classify(FreqDist(clean))\n",
    "    print(res)\n",
    "    \n",
    "rev = \"The plane is late and the flight attendant was rude. It's terrible\"\n",
    "analyze(rev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid\n"
     ]
    }
   ],
   "source": [
    "def inputTxt():\n",
    "    rev = input(\"Enter word\")\n",
    "    \n",
    "    if(len(rev.split())>=5):\n",
    "        print(\"valid\")\n",
    "        return rev\n",
    "    else:\n",
    "        print(\"invalid\")\n",
    "        return rev\n",
    "    \n",
    "inputs = inputTxt()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
