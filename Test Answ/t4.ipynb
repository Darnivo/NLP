{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk, pickle, pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import wordnet, stopwords\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.classify import NaiveBayesClassifier,accuracy\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.stem import PorterStemmer,WordNetLemmatizer\n",
    "\n",
    "from string import punctuation\n",
    "from random import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "stems = PorterStemmer()\n",
    "lemms = WordNetLemmatizer()\n",
    "stopW = stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preproc(words):\n",
    "    tokens = word_tokenize(words)\n",
    "    \n",
    "    words = [word.lower() for word in tokens if word.isalpha()]\n",
    "    words = [word for word in words if word not in stopW]\n",
    "    \n",
    "    words = [stems.stem(word) for word in words]\n",
    "    words = [lemms.lemmatize(word) for word in words]\n",
    "    \n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = pd.read_csv(\"./dataset.csv\",encoding=\"utf-8-sig\")\n",
    "\n",
    "all_w = []\n",
    "for sent in ds[\"text\"]:\n",
    "    for word in preproc(sent):\n",
    "        all_w.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "fd = FreqDist(all_w)\n",
    "commonW = {word for word in fd.most_common(1000)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_set = []\n",
    "\n",
    "for txt,lbl in zip(ds[\"text\"],ds[\"label\"]):\n",
    "    feature = {}\n",
    "    \n",
    "    proc_txt = preproc(txt)\n",
    "    \n",
    "    for word in proc_txt:\n",
    "        feature[word] = (word in commonW)\n",
    "        \n",
    "    feature_set.append((feature,lbl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffle(feature_set)\n",
    "\n",
    "idx = int(len(feature_set)*0.7)\n",
    "trains = feature_set[:idx]\n",
    "tests = feature_set[idx:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "                 terribl = False          negati : positi =     10.1 : 1.0\n",
      "                    hope = False          negati : positi =      8.2 : 1.0\n",
      "                 airport = False          positi : negati =      6.9 : 1.0\n",
      "                 fantast = False          positi : negati =      6.9 : 1.0\n",
      "                  immedi = False          negati : positi =      6.4 : 1.0\n"
     ]
    }
   ],
   "source": [
    "clf = NaiveBayesClassifier.train(trains)\n",
    "clf.show_most_informative_features(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def synanto(words):\n",
    "    SYN = []\n",
    "    ANT = []\n",
    "    \n",
    "    for syns in wordnet.synsets(words):\n",
    "        for lemmas in syns.lemmas():\n",
    "            SYN.append(lemmas.name())\n",
    "            for ants in lemmas.antonyms():\n",
    "                ANT.append(ants.name())\n",
    "    return SYN,ANT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the\n",
      "-\n",
      "-\n",
      "==========\n",
      "book\n",
      "book, book, volume, record, record_book, book, script, book, playscript, ledger, leger, account_book, book_of_account, book, book, book, rule_book, Koran, Quran, al-Qur'an, Book, Bible, Christian_Bible, Book, Good_Book, Holy_Scripture, Holy_Writ, Scripture, Word_of_God, Word, book, book, book, reserve, hold, book, book, book\n",
      "-\n",
      "==========\n",
      "is\n",
      "be, be, be, exist, be, be, equal, be, constitute, represent, make_up, comprise, be, be, follow, embody, be, personify, be, be, live, be, cost, be\n",
      "differ\n",
      "==========\n",
      "good\n",
      "good, good, goodness, good, goodness, commodity, trade_good, good, good, full, good, good, estimable, good, honorable, respectable, beneficial, good, good, good, just, upright, adept, expert, good, practiced, proficient, skillful, skilful, good, dear, good, near, dependable, good, safe, secure, good, right, ripe, good, well, effective, good, in_effect, in_force, good, good, serious, good, sound, good, salutary, good, honest, good, undecomposed, unspoiled, unspoilt, good, well, good, thoroughly, soundly, good\n",
      "evil, evilness, bad, badness, bad, evil, ill\n",
      "==========\n"
     ]
    }
   ],
   "source": [
    "rev = \"the book is good\"\n",
    "\n",
    "tok_rev = word_tokenize(rev)\n",
    "for words in tok_rev:\n",
    "    syn,ant = synanto(words)\n",
    "    print(words)\n",
    "    print(f\"{\", \".join(syn) if syn else \"-\"}\")\n",
    "    print(f\"{\", \".join(ant) if ant else \"-\"}\")\n",
    "    \n",
    "    # print(ant)\n",
    "    print(\"=\"*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tagger(words):\n",
    "    tok_rev = word_tokenize(words)\n",
    "    tags = pos_tag(tok_rev)\n",
    "    return tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. i => NNS\n",
      "2. like => VBP\n",
      "3. to => TO\n",
      "4. read => VB\n",
      "5. books => NNS\n"
     ]
    }
   ],
   "source": [
    "rev = \"i like to read books\"\n",
    "\n",
    "tag_rev = tagger(rev)\n",
    "\n",
    "for i,word in enumerate(tag_rev):\n",
    "    print(f\"{i+1}. {word[0]} => {word[1]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive\n"
     ]
    }
   ],
   "source": [
    "def analyze(words):\n",
    "    file = open(\"./modelt3.pickle\",\"rb\")\n",
    "    classer = pickle.load(file)\n",
    "    file.close()\n",
    "    \n",
    "    cleaned = preproc(words)\n",
    "    \n",
    "    res = classer.classify(FreqDist(cleaned))\n",
    "    \n",
    "    print(res)\n",
    "\n",
    "\n",
    "rev = \"i am an envoy of god who shall bring destruction upon your lands\"\n",
    "analyze(rev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive\n"
     ]
    }
   ],
   "source": [
    "test = ds[ds[\"text\"] == \"Friendly staff, same starbucks fair you get anywhere else.  Sometimes the lines can get long.\"]\n",
    "\n",
    "for lbl in test[\"label\"]:\n",
    "    print(lbl)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
