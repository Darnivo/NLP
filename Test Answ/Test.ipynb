{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "negative\n",
      "Most Informative Features\n",
      "                terrible = True           negati : positi =     12.1 : 1.0\n",
      "               excellent = True           positi : negati =      8.8 : 1.0\n",
      "                 perfect = True           positi : negati =      8.8 : 1.0\n",
      "                horrible = True           negati : positi =      8.5 : 1.0\n",
      "                   loved = True           positi : negati =      8.2 : 1.0\n",
      "0.7272727272727273\n"
     ]
    }
   ],
   "source": [
    "# Check for file\n",
    "\n",
    "if os.path.exists(\"./model.pickle\"):\n",
    "    #if it exists,load the data\n",
    "    model = open('./model.pickle','rb')\n",
    "    classifier = pickle.load(model)\n",
    "    model.close()\n",
    "    \n",
    "    #then, display 5 most informative features\n",
    "    classifier.show_most_informative_features(5)\n",
    "else:\n",
    "    # If it does not exist, Train a model\n",
    "    \n",
    "    \n",
    "    # 1. Load the dataset from the csv file\n",
    "    dataset = []\n",
    "\n",
    "    with open('./dataset.csv', encoding= \"utf-8-sig\") as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        for row in reader:\n",
    "            dataset.append((row['text'],row['label']))\n",
    "    #                           ^  its the header of the file\n",
    "    \n",
    "    # 2. Preprocess the data\n",
    "        # Tokenize the data\n",
    "    from nltk.tokenize import word_tokenize\n",
    "        \n",
    "    list_words = []\n",
    "\n",
    "    for sentence, label in dataset:\n",
    "        words = word_tokenize(sentence)\n",
    "        list_words.extend(words)\n",
    "        \n",
    "        # Remove stopwords, symbol & numbers\n",
    "    from nltk.corpus import stopwords\n",
    "\n",
    "    def clean_text(input):\n",
    "        # make lower\n",
    "        input = [word.lower() for word in input]\n",
    "        # remove symbols\n",
    "        input = [word for word in input if word.isalpha()]\n",
    "        # remove stopwords\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        input = [word for word in input if not word in stop_words]\n",
    "        # remove numbers\n",
    "        input = [word for word in input if not word.isdigit()] ##! isnumeric() can also work\n",
    "        # remove punctuation\n",
    "        from string import punctuation\n",
    "        input = [word for word in input if not word in punctuation]\n",
    "        return input\n",
    "    \n",
    "        #clean data w/ func\n",
    "    list_words = clean_text(list_words)\n",
    "        \n",
    "        # lemmatize / stem the data\n",
    "        \n",
    "        # lemmatize:\n",
    "    from nltk.stem import WordNetLemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "        \n",
    "    def lemmatize(input):\n",
    "        lemmas = [lemmatizer.lemmatize(word) for word in input]\n",
    "        return lemmas\n",
    "    \n",
    "    list_words = lemmatize(list_words)\n",
    "    \n",
    "    \n",
    "    # Model training (?)\n",
    "        # A. use nltk.FreqDist to get the most common words\n",
    "    from nltk.probability import  FreqDist\n",
    "    fd = FreqDist(list_words)\n",
    "    # use the 1000 most common words as features\n",
    "    list_words = [word for word, count in fd.most_common(1000)]\n",
    "    \n",
    "        # B. Create a feature set\n",
    "    processed_dataset = []\n",
    "    for sentence, label in dataset:\n",
    "        words = word_tokenize(sentence)\n",
    "        words = clean_text(words)\n",
    "        words = lemmatize(words)\n",
    "        dict_features = {}\n",
    "        for feature in list_words:\n",
    "            dict_features[feature] = (feature in words)\n",
    "        processed_dataset.append((dict_features,label))\n",
    "        \n",
    "        # C. Split the dataset into training and testing\n",
    "    import random\n",
    "    \n",
    "    random.shuffle(processed_dataset)\n",
    "    split_index = int(len(processed_dataset)* 0.7)\n",
    "    training_data = processed_dataset[:split_index]\n",
    "    testing_data = processed_dataset[split_index:]\n",
    "    \n",
    "        # D. Train the model\n",
    "    from nltk.classify import NaiveBayesClassifier, accuracy\n",
    "    classifier = NaiveBayesClassifier.train(training_data)\n",
    "        \n",
    "    \n",
    "    \n",
    "        # 3. Classify reviews as either Positive or Negative\n",
    "    test_input = \"Absolutely blown away by the food here! Service was top-notch, and the ambiance made it feel like I was dining in Italy. I will definitely be coming back!\"\n",
    "    test_input_token = word_tokenize(test_input)\n",
    "    test_input_token = clean_text(test_input_token)\n",
    "    test_input_token = lemmatize(test_input_token)\n",
    "    \n",
    "    test_input_result = classifier.classify(FreqDist(test_input_token))\n",
    "    print(test_input_result)\n",
    "    \n",
    "        # 4.  Show the 5 most informative features and the training accuracy\n",
    "    classifier.show_most_informative_features(5)    \n",
    "    print(accuracy(classifier,testing_data))\n",
    "    \n",
    "        # 5. Save the model as model.pickle\n",
    "    model = open('./model.pickle','wb')\n",
    "    pickle.dump(classifier,model)\n",
    "    model.close()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ##! check how many in fd\n",
    "\n",
    "# print(len(fd))\n",
    "\n",
    "# ##! how to see all of fd\n",
    "\n",
    "# for word, frequency in fd.most_common():\n",
    "#     print(f'{word}: {frequency}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print (list_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Application Menu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Enter a review\n",
      "2. Analyze Your Review\n",
      "3. Exit\n",
      "Curr stored input:\n",
      "Sentence input successful\n",
      "1. Enter a review\n",
      "2. Analyze Your Review\n",
      "3. Exit\n",
      "Curr stored input:you hate to see it burn\n",
      "Tagged review: [('you', 'PRP'), ('hate', 'VBP'), ('to', 'TO'), ('see', 'VB'), ('it', 'PRP'), ('burn', 'VB')]\n",
      "you:\n",
      "Synonyms: -\n",
      "Antonyms: -\n",
      "hate:\n",
      "Synonyms: {'hate', 'detest', 'hatred'}\n",
      "Antonyms: {'love'}\n",
      "to:\n",
      "Synonyms: -\n",
      "Antonyms: -\n",
      "see:\n",
      "Synonyms: {'insure', 'image', 'watch', 'construe', 'attend', 'go_steady', 'escort', 'date', 'realise', 'learn', 'view', 'get_word', 'regard', 'catch', 'determine', 'go_through', 'consider', 'get_a_line', 'take_in', 'visualize', 'visualise', 'pick_up', 'go_out', 'come_across', 'project', 'get_wind', 'see', 'find_out', 'understand', 'discover', 'interpret', 'control', 'ensure', 'check', 'visit', 'encounter', 'run_into', 'reckon', 'picture', 'realize', 'look', 'experience', 'meet', 'examine', 'figure', 'hear', 'ascertain', 'envision', 'see_to_it', 'take_care', 'witness', 'find', 'assure', 'fancy', 'run_across'}\n",
      "Antonyms: -\n",
      "it:\n",
      "Synonyms: {'IT', 'information_technology'}\n",
      "Antonyms: -\n",
      "burn:\n",
      "Synonyms: {'cut', 'glow', 'burn_mark', 'combust', 'bite', 'cauterize', 'suntan', 'burn_down', 'incinerate', 'burn', 'sting', 'burn_off', 'burn_up', 'cauterise', 'fire', 'tan', 'burning', 'sunburn'}\n",
      "Antonyms: -\n",
      "The review is marked as positive\n",
      "1. Enter a review\n",
      "2. Analyze Your Review\n",
      "3. Exit\n",
      "Curr stored input:you hate to see it burn\n",
      "Sentence input successful\n",
      "1. Enter a review\n",
      "2. Analyze Your Review\n",
      "3. Exit\n",
      "Curr stored input:i hate everything about this place\n",
      "Tagged review: [('i', 'JJ'), ('hate', 'VBP'), ('everything', 'NN'), ('about', 'IN'), ('this', 'DT'), ('place', 'NN')]\n",
      "i:\n",
      "Synonyms: {'iodine', 'one', 'i', 'single', 'unity', 'ane', 'iodin', 'I', '1', 'atomic_number_53', 'ace'}\n",
      "Antonyms: -\n",
      "hate:\n",
      "Synonyms: {'hate', 'detest', 'hatred'}\n",
      "Antonyms: {'love'}\n",
      "everything:\n",
      "Synonyms: -\n",
      "Antonyms: -\n",
      "about:\n",
      "Synonyms: {'astir', 'nigh', 'approximately', 'close_to', 'almost', 'roughly', 'near', 'around', 'more_or_less', 'some', 'or_so', 'most', 'about', 'virtually', 'just_about', 'well-nigh', 'nearly'}\n",
      "Antonyms: -\n",
      "this:\n",
      "Synonyms: -\n",
      "Antonyms: -\n",
      "place:\n",
      "Synonyms: {'seat', 'stead', 'home', 'target', 'place', 'pose', 'post', 'property', 'shoes', 'send', 'rank', 'rate', 'invest', 'station', 'direct', 'commit', 'come_in', 'billet', 'spot', 'topographic_point', 'space', 'order', 'point', 'site', 'locate', 'localise', 'position', 'plaza', 'localize', 'blank_space', 'range', 'office', 'berth', 'put', 'lay', 'lieu', 'identify', 'piazza', 'situation', 'set', 'grade', 'come_out', 'aim'}\n",
      "Antonyms: {'divest'}\n",
      "The review is marked as positive\n",
      "1. Enter a review\n",
      "2. Analyze Your Review\n",
      "3. Exit\n",
      "Curr stored input:i hate everything about this place\n",
      "Exiting program...\n"
     ]
    }
   ],
   "source": [
    "# Make a menu with 3 options\n",
    "\n",
    "review = \"\"\n",
    "\n",
    "while True:\n",
    "    print(\"1. Enter a review\")\n",
    "    print(\"2. Analyze Your Review\")\n",
    "    print(\"3. Exit\")\n",
    "    \n",
    "    ##! enable for debugging\n",
    "    print(\"Curr stored input:\" + review)\n",
    "    \n",
    "    choice = input(\"Enter your choice: \")\n",
    "\n",
    "    \n",
    "    if choice == '1':\n",
    "        review = input(\"Enter your review: \")\n",
    "        \n",
    "        print(\"Sentence input successful\")\n",
    "    elif choice == '2':\n",
    "        # If no review has been entered, return to the menu\n",
    "        if review == \"\":\n",
    "            print(\"No review has been entered. Please enter a review first.\")\n",
    "            continue\n",
    "        # Preprocess the review\n",
    "        else:\n",
    "            # Make a copy of the review for analysis\n",
    "            review_step2 = review\n",
    "            \n",
    "            # Remove unwanted characters\n",
    "            review_step2 = word_tokenize(review_step2)\n",
    "            # Remove symbols & numbers\n",
    "            review_step2 = [word for word in review_step2 if word.isalpha()]\n",
    "            review_step2 = [word for word in review_step2 if not word.isdigit()]\n",
    "            \n",
    "            # Display POS tagging\n",
    "            from nltk.tag import pos_tag\n",
    "            tagged_review = pos_tag(review_step2)\n",
    "            print(\"Tagged review:\", tagged_review)\n",
    "        \n",
    "            # Display synonyms and antonyms for each word\n",
    "            from nltk.corpus import wordnet\n",
    "            \n",
    "            for word in review_step2:\n",
    "                synonyms = []\n",
    "                antonyms = []\n",
    "                for syn in wordnet.synsets(word):\n",
    "                    for lemma in syn.lemmas():\n",
    "                        synonyms.append(lemma.name())\n",
    "                        if lemma.antonyms():\n",
    "                            antonyms.append(lemma.antonyms()[0].name())\n",
    "                print(word + \":\")\n",
    "                print(\"Synonyms:\", set(synonyms) if synonyms else \"-\")\n",
    "                print(\"Antonyms:\", set(antonyms) if antonyms else \"-\")\n",
    "            \n",
    "            # Do sentiment analysis\n",
    "            review_step2 = clean_text(review_step2)\n",
    "            review_step2 = lemmatize(review_step2)\n",
    "            \n",
    "            result = classifier.classify(FreqDist(review_step2))\n",
    "            print(\"The review is marked as\", result)\n",
    "            \n",
    "    elif choice == '3':\n",
    "        print(\"Exiting program...\")\n",
    "        break\n",
    "    else:\n",
    "        print(\"Invalid choice. Please try again.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
