{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk, pickle, pandas as pd\n",
    "from string import punctuation\n",
    "from random import shuffle\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import wordnet, stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.classify import NaiveBayesClassifier,accuracy\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.probability import FreqDist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "stems = PorterStemmer()\n",
    "lemms = WordNetLemmatizer()\n",
    "stopW = stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(words):\n",
    "    tokens = word_tokenize(words)\n",
    "    \n",
    "    #no sym no num\n",
    "    words = [word.lower() for word in tokens if word.isalpha()]\n",
    "\n",
    "    #no stop\n",
    "    words = [word for word in words if word not in stopW]\n",
    "    \n",
    "    #stem & lem\n",
    "    words = [stems.stem(word)for word in words]\n",
    "    words = [lemms.lemmatize(word)for word in words]\n",
    "    \n",
    "    #return\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"./dataset.csv\",encoding=\"utf-8-sig\")\n",
    "\n",
    "all_w = []\n",
    "for sent in dataset[\"text\"]:\n",
    "    for word in preprocess(sent):\n",
    "        all_w.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "fd = FreqDist(all_w)\n",
    "common_w = {word for word in fd.most_common(1000)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_set = []\n",
    "for text,category in zip(dataset[\"text\"],dataset[\"label\"]):\n",
    "    feature = {}\n",
    "    \n",
    "    processed_feature = preprocess(text)\n",
    "    \n",
    "    for word in processed_feature:\n",
    "        feature[word] = (word in common_w)\n",
    "    feature_set.append((feature,category))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffle(feature_set)\n",
    "\n",
    "split_idx = int(len(feature_set)*0.7)\n",
    "trains = feature_set[:split_idx]\n",
    "tests = feature_set[split_idx:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "                  someon = False          negati : positi =     12.5 : 1.0\n",
      "                 terribl = False          negati : positi =     11.9 : 1.0\n",
      "                 perfect = False          positi : negati =     10.7 : 1.0\n",
      "                 horribl = False          negati : positi =      9.3 : 1.0\n",
      "                    bill = False          negati : positi =      6.8 : 1.0\n"
     ]
    }
   ],
   "source": [
    "classifier = NaiveBayesClassifier.train(trains)\n",
    "classifier.show_most_informative_features(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6727272727272727\n"
     ]
    }
   ],
   "source": [
    "print (accuracy(classifier,tests))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"modelt3.pickle\",\"wb\")\n",
    "pickle.dump(classifier,file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def synanto(words):\n",
    "    SYN = []\n",
    "    ANT = []\n",
    "    \n",
    "    for syns in wordnet.synsets(words):\n",
    "        for lemmas in syns.lemmas():\n",
    "            SYN.append(lemmas.name())\n",
    "            for ants in lemmas.antonyms():\n",
    "                ANT.append(ants.name())\n",
    "    return SYN,ANT\n",
    "\n",
    "def tagger(words):\n",
    "    tokens = word_tokenize(words)\n",
    "    tags = pos_tag(tokens)\n",
    "    return tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the\n",
      "-\n",
      "-\n",
      "==========\n",
      "book\n",
      "book, book, volume, record, record_book, book, script, book, playscript, ledger, leger, account_book, book_of_account, book, book, book, rule_book, Koran, Quran, al-Qur'an, Book, Bible, Christian_Bible, Book, Good_Book, Holy_Scripture, Holy_Writ, Scripture, Word_of_God, Word, book, book, book, reserve, hold, book, book, book\n",
      "-\n",
      "==========\n",
      "is\n",
      "be, be, be, exist, be, be, equal, be, constitute, represent, make_up, comprise, be, be, follow, embody, be, personify, be, be, live, be, cost, be\n",
      "differ\n",
      "==========\n",
      "good\n",
      "good, good, goodness, good, goodness, commodity, trade_good, good, good, full, good, good, estimable, good, honorable, respectable, beneficial, good, good, good, just, upright, adept, expert, good, practiced, proficient, skillful, skilful, good, dear, good, near, dependable, good, safe, secure, good, right, ripe, good, well, effective, good, in_effect, in_force, good, good, serious, good, sound, good, salutary, good, honest, good, undecomposed, unspoiled, unspoilt, good, well, good, thoroughly, soundly, good\n",
      "evil, evilness, bad, badness, bad, evil, ill\n",
      "==========\n"
     ]
    }
   ],
   "source": [
    "rev = \"the book is good\"\n",
    "\n",
    "tok_rev = word_tokenize(rev)\n",
    "for words in tok_rev:\n",
    "    syn,ant = synanto(words)\n",
    "    print(words)\n",
    "    print(f\"{\", \".join(syn) if syn else \"-\"}\")\n",
    "    print(f\"{\", \".join(ant) if ant else \"-\"}\")\n",
    "    print(\"=\"*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. i => NNS\n",
      "2. like => VBP\n",
      "3. to => TO\n",
      "4. read => VB\n",
      "5. books => NNS\n"
     ]
    }
   ],
   "source": [
    "rev = \"i like to read books\"\n",
    "\n",
    "tag_rev = tagger(rev)\n",
    "\n",
    "for i,word in enumerate(tag_rev):\n",
    "    print(f\"{i+1}. {word[0]} => {word[1]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# w count for input\n",
    "\n",
    "rev = \"\"\n",
    "\n",
    "while True:\n",
    "    rev = input(\"Enter input \")\n",
    "    if(len(rev.split()))>2:\n",
    "        break\n",
    "    else:\n",
    "        print(\"need more words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive\n"
     ]
    }
   ],
   "source": [
    "def analyze(words):\n",
    "    cleaned = preprocess(words)\n",
    "    \n",
    "    file = open(\"./modelt3.pickle\",\"rb\")\n",
    "    classer = pickle.load(file)\n",
    "    file.close()\n",
    "    \n",
    "    result = classer.classify(FreqDist(cleaned)) \n",
    "    \n",
    "    print(result)\n",
    "\n",
    "\n",
    "rev = \"The food is so terrible\"\n",
    "analyze(rev)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
