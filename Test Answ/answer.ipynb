{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all the libraries needed\n",
    "import nltk # Natural Language Processing Library\n",
    "import pickle # For The Model\n",
    "import string # For String Operations\n",
    "import pandas as pd # For DataFrame\n",
    "\n",
    "# Functions from the libraries\n",
    "from nltk import FreqDist\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.tag import pos_tag\n",
    "from random import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to d:\\NLP\n",
      "[nltk_data]     Quiz1\\.venv\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to d:\\NLP\n",
      "[nltk_data]     Quiz1\\.venv\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to d:\\NLP\n",
      "[nltk_data]     Quiz1\\.venv\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to d:\\NLP\n",
      "[nltk_data]     Quiz1\\.venv\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"averaged_perceptron_tagger\")\n",
    "nltk.download(\"wordnet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting variables\n",
    "stemmer = PorterStemmer()\n",
    "wnl = WordNetLemmatizer()\n",
    "eng_stopwords = stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(document):\n",
    "\t# Change into lowercase and tokenize\n",
    "\twords = word_tokenize(document.lower())\n",
    "\t\n",
    "\t# Lemmatizing and Stemming\n",
    "\twords = [wnl.lemmatize(word) for word in words]\n",
    "\twords = [stemmer.stem(word) for word in words]\n",
    "\n",
    "\t# Check if words are not in stop_words and only consists of alphabetic\n",
    "\treturn {word: True for word in words if word not in eng_stopwords and word.isalpha()}\n",
    "\n",
    "def trainModel():\n",
    "\tdataset = pd.read_csv(\"dataset.csv\")\n",
    "\n",
    "\tfeatures_sets = [(preprocessing(text), label) for text, label in zip(dataset[\"text\"], dataset[\"label\"])]\n",
    "\n",
    "\tshuffle(features_sets)\n",
    "\n",
    "\tsplit_index = int(len(features_sets) * .85)\n",
    "\ttrain_set, test_set = features_sets[:split_index], features_sets[split_index:]\n",
    "\n",
    "\t# Training the model (Naive Bayes)\n",
    "\tclassifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "\n",
    "\t# Testing accuracy\n",
    "\taccuracy = nltk.classify.accuracy(classifier, test_set)\n",
    "\tprint(\"Accuracy: \", accuracy)\n",
    "\n",
    "\t# Print 5 most informative features\n",
    "\tclassifier.show_most_informative_features(5)\n",
    "\n",
    "\t# Save the trained model using pickle\n",
    "\tfile = open(\"model.pickle\", \"wb\")\n",
    "\tpickle.dump(classifier, file)\n",
    "\tfile.close()\n",
    "\n",
    "\treturn classifier\n",
    "\n",
    "def readModel():\n",
    "    # Check the model is available or not\n",
    "    \n",
    "    # If the model is available\n",
    "\ttry:\n",
    "\t\tfile = open(\"model.pickle\", \"rb\") # Read Binary\n",
    "\t\tprint(\"Model is available!\")\n",
    "\t\t# Read Model\n",
    "\t\tprint(\"Loading the model...\")\n",
    "\t\tclassifier = pickle.load(file)\n",
    "\t\tfile.close()\n",
    "\n",
    "\t\tprint(\"Model load succesfully!\")\n",
    "\t\tclassifier.show_most_informative_features(5)\n",
    "  \n",
    "\t# Else (model unvailable)\n",
    "\texcept:\n",
    "\t\tprint(\"Model is not available!\")\n",
    "\t\tprint(\"Preparing for model training!\")\n",
    "\t\tclassifier = trainModel()\n",
    "  \n",
    "\treturn classifier\n",
    "\n",
    "def writeReview():\n",
    "    while True:\n",
    "        review = input(\"Input your review [>= 2 words]: \")\n",
    "        \n",
    "        words = review.split()\n",
    "        \n",
    "        if len(words) > 1:\n",
    "            print(\"Review added!\")\n",
    "            return review\n",
    "        else:\n",
    "            print(\"Your review must consisst of at least 2 words!\")\n",
    "     \n",
    "def analyzeReview(review, classifier):\n",
    "    if len(review) == 0:\n",
    "        print(\"Review is empty!\")\n",
    "        return\n",
    "    \n",
    "    # Tokenizing\n",
    "    words = word_tokenize(review.lower())\n",
    "    \n",
    "    # Frequency Distribution\n",
    "    words = FreqDist([word for word in words if word.isalpha() and word not in string.punctuation])\n",
    "    \n",
    "    # Tagging\n",
    "    tagged = pos_tag(words)\n",
    "    \n",
    "    print(\"Review Part of Speech Tag: \")\n",
    "    \n",
    "    for i, word in enumerate(tagged):\n",
    "        print(f\"{i+1}. {word[0]}, {word[1]}\")\n",
    "    \n",
    "    # Synonym and Antonym\n",
    "    for word in words:\n",
    "        print(\"==============\")\n",
    "        print(f\"Word: {word}\")\n",
    "        print(\"==============\")\n",
    "        \n",
    "        # Synsets\n",
    "        synsets = wordnet.synsets(word)\n",
    "        synonyms = []\n",
    "        antonyms = []\n",
    "        \n",
    "        for synset in synsets:\n",
    "            for lemma in synset.lemmas():\n",
    "                 synonyms.append(lemma.name())\n",
    "                 for antonym in lemma.antonyms():\n",
    "                     antonyms.append(antonym.name())\n",
    "        \n",
    "        print(\"Synonyms: \")\n",
    "        \n",
    "        if len(synonyms) == 0:\n",
    "            print(\"No synonym detected!\")\n",
    "        else:\n",
    "            for syn in synonyms[:5]:\n",
    "                print(f\"(+){syn}\")\n",
    "                \n",
    "        print(\"Antonyms: \")\n",
    "        \n",
    "        if len(antonyms) == 0:\n",
    "            print(\"No antonym detected!\")\n",
    "        else:\n",
    "            for antonym in antonyms[:5]:\n",
    "                print(f\"(-){antonym}\")\n",
    "                \n",
    "        print(\"===========================\")    \n",
    "\n",
    "\t# Predict the review\n",
    "    \n",
    "    # Preprocessing to remove punctuation and eng_stopwords and tokenize it\n",
    "    clean_review = [word for word in word_tokenize(review) if word not in string.punctuation and word not in eng_stopwords]\n",
    "    \n",
    "    # clean_review = [wnl.lemmatize(review) for review in clean_review]\n",
    "    # clean_review = [stemmer.stem(review) for review in clean_review]\n",
    "    \n",
    "    # Preprocessing to lemmatize and stemming the words\n",
    "    clean_review = [wnl.lemmatize(stemmer.stem(word)) for word in clean_review] \t\n",
    "    \n",
    "    result = classifier.classify(FreqDist(clean_review))\n",
    "    \n",
    "    print(f\"Your Review: {review}\")\n",
    "    print(f\"Review Category: {result}\")\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.6024096385542169\n",
      "Most Informative Features\n",
      "                 terribl = True           negati : positi =     12.5 : 1.0\n",
      "                 horribl = True           negati : positi =      9.4 : 1.0\n",
      "                    rude = True           negati : positi =      8.8 : 1.0\n",
      "                    wors = True           negati : positi =      7.0 : 1.0\n",
      "                 perfect = True           positi : negati =      6.8 : 1.0\n"
     ]
    }
   ],
   "source": [
    "# Train the classifier and dump it into model.pickle\n",
    "classifier = trainModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['text', 'label'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "dataset = pd.read_csv(\"dataset.csv\")\n",
    "print(dataset.columns)  # This will print all column names in the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model is available!\n",
      "Loading the model...\n",
      "Model load succesfully!\n",
      "Most Informative Features\n",
      "                 terribl = True           negati : positi =     12.5 : 1.0\n",
      "                 horribl = True           negati : positi =      9.4 : 1.0\n",
      "                    rude = True           negati : positi =      8.8 : 1.0\n",
      "                    wors = True           negati : positi =      7.0 : 1.0\n",
      "                 perfect = True           positi : negati =      6.8 : 1.0\n",
      "Food Review Sentiment Analysis\n",
      "Your Review:  No Review\n",
      "1. Write your review\n",
      "2. Analyze your review\n",
      "3. Exit\n",
      ">> \n",
      "Review added!\n",
      "Food Review Sentiment Analysis\n",
      "Your Review:  i love planes\n",
      "1. Write your review\n",
      "2. Analyze your review\n",
      "3. Exit\n",
      ">> \n",
      "Review Part of Speech Tag: \n",
      "1. i, NN\n",
      "2. love, VBP\n",
      "3. planes, NNS\n",
      "==============\n",
      "Word: i\n",
      "==============\n",
      "Synonyms: \n",
      "(+)iodine\n",
      "(+)iodin\n",
      "(+)I\n",
      "(+)atomic_number_53\n",
      "(+)one\n",
      "Antonyms: \n",
      "No antonym detected!\n",
      "===========================\n",
      "==============\n",
      "Word: love\n",
      "==============\n",
      "Synonyms: \n",
      "(+)love\n",
      "(+)love\n",
      "(+)passion\n",
      "(+)beloved\n",
      "(+)dear\n",
      "Antonyms: \n",
      "(-)hate\n",
      "(-)hate\n",
      "===========================\n",
      "==============\n",
      "Word: planes\n",
      "==============\n",
      "Synonyms: \n",
      "(+)airplane\n",
      "(+)aeroplane\n",
      "(+)plane\n",
      "(+)plane\n",
      "(+)sheet\n",
      "Antonyms: \n",
      "No antonym detected!\n",
      "===========================\n",
      "Your Review: i love planes\n",
      "Review Category: positive\n",
      "Food Review Sentiment Analysis\n",
      "Your Review:  i love planes\n",
      "1. Write your review\n",
      "2. Analyze your review\n",
      "3. Exit\n",
      ">> \n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: ''",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 15\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m3. Exit\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m>> \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 15\u001b[0m choice \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m>> \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (choice \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m     17\u001b[0m \treview \u001b[38;5;241m=\u001b[39m writeReview()\n",
      "\u001b[1;31mValueError\u001b[0m: invalid literal for int() with base 10: ''"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\t# Read Model\n",
    "\tclassifier = readModel()\n",
    "\t\n",
    "\t# Review\n",
    "\treview = \"\"\n",
    " \n",
    "\twhile True:\n",
    "\t\tprint(\"Food Review Sentiment Analysis\")\n",
    "\t\tprint(\"Your Review: \", \"No Review\" if len(review) == 0 else review)\n",
    "\t\tprint(\"1. Write your review\")\n",
    "\t\tprint(\"2. Analyze your review\")\n",
    "\t\tprint(\"3. Exit\")\n",
    "\t\tprint(\">> \")\n",
    "\t\tchoice = int(input(\">> \"))\n",
    "\t\tif (choice == 1):\n",
    "\t\t\treview = writeReview()\n",
    "\t\telif (choice == 2):\n",
    "\t\t\tanalyzeReview(review, classifier)\n",
    "\t\telif (choice == 3):\n",
    "\t\t\tprint(\"Thanks for using this application!\")\n",
    "\t\t\tbreak\n",
    "\t\telse:\n",
    "\t\t\tprint(\"Please only choose the available menu [1-3]!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
