{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk, pickle, pandas as pd\n",
    "from nltk import pos_tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.classify import NaiveBayesClassifier, accuracy \n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from random import shuffle\n",
    "\n",
    "porter = PorterStemmer()\n",
    "lem = WordNetLemmatizer()\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(word):\n",
    "    token = word_tokenize(word)\n",
    "    filter = [word.lower() for word in token if word.isalpha()]\n",
    "    \n",
    "    nostop = [word for word in filter if word not in stop_words]\n",
    "    \n",
    "    res = [porter.stem(word) for word in nostop]\n",
    "    res = [lem.lemmatize(word) for word in res]\n",
    "    \n",
    "    return res "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = pd.read_csv(\"dataset.csv\", encoding = \"utf-8-sig\")\n",
    "\n",
    "all_words = []\n",
    "for sentence in ds[\"text\"]:\n",
    "    for word in preprocess(sentence):\n",
    "        all_words.append(word)\n",
    "        \n",
    "fd = FreqDist(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_words = {word for word in fd.most_common(100)}\n",
    "feature_set = []\n",
    "\n",
    "for text, category in zip(ds[\"text\"], ds[\"label\"]):\n",
    "    feature = {}\n",
    "    \n",
    "    processed_feature = preprocess(text)\n",
    "    \n",
    "    for word in processed_feature:\n",
    "        feature[word] = (word in common_words)\n",
    "    feature_set.append((feature, category))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "                 perfect = False          positi : negati =     13.3 : 1.0\n",
      "                 terribl = False          negati : positi =     10.6 : 1.0\n",
      "                    sign = False          negati : positi =      9.9 : 1.0\n",
      "                 horribl = False          negati : positi =      8.6 : 1.0\n",
      "                 possibl = False          negati : positi =      8.6 : 1.0\n"
     ]
    }
   ],
   "source": [
    "shuffle(feature_set)\n",
    "\n",
    "split = int(len(feature_set)*0.8)\n",
    "train = feature_set[:split]\n",
    "test = feature_set[split:]\n",
    "\n",
    "clf = NaiveBayesClassifier.train(train)\n",
    "clf.show_most_informative_features(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy =  75.45454545454545 %\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy = \", accuracy(clf, test)*100, \"%\")\n",
    "\n",
    "file = open(\"model.pickle\", \"wb\")\n",
    "pickle.dump(clf, file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def showSynAnt(word):\n",
    "    SYN = []\n",
    "    ANT = []\n",
    "    \n",
    "    for syn in wordnet.synsets(word):\n",
    "        for lemma in syn.lemmas():\n",
    "            SYN.append(lemma.name())\n",
    "            if lemma.antonyms():\n",
    "                ANT.append(lemma.antonyms()[0].name())\n",
    "                \n",
    "    return SYN, ANT\n",
    "                \n",
    "def tagger(word):\n",
    "    token = word_tokenize(word)\n",
    "    tag = pos_tag(token)\n",
    "    return tag "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "review = []\n",
    "\n",
    "def wReview():\n",
    "    rev = input(\"Insert the review : \") \n",
    "    if len(rev.split()) >= 2:\n",
    "        review.append(rev)\n",
    "        print(\"append success\")\n",
    "    else:\n",
    "        print(\"invalid\")\n",
    "        \n",
    "def analyze():\n",
    "    if not review:\n",
    "        print(\"No review exist\")\n",
    "        \n",
    "    REV = review[-1]\n",
    "    REV_TAG = tagger(REV)  \n",
    "   \n",
    "    for word, pos in REV_TAG:\n",
    "        synonyms, antonyms = showSynAnt(word)\n",
    "        print(f\"Word : {word}, POS : {pos}\")\n",
    "        print(f\"Syn : {', '.join(synonyms) if synonyms else 'None'}\")\n",
    "        print(f\"Ant : {', '.join(antonyms) if antonyms else 'None'}\")\n",
    "\n",
    "    print(\"Category : \")\n",
    "    file = open(\"model.pickle\", \"rb\")\n",
    "    clf = pickle.load(file)\n",
    "    file.close()\n",
    "    token = word_tokenize(REV)\n",
    "    res = clf.classify(FreqDist(token))    \n",
    "    print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "invalid\n",
      "No review exist\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m wReview()\n\u001b[1;32m----> 2\u001b[0m \u001b[43manalyze\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[8], line 15\u001b[0m, in \u001b[0;36manalyze\u001b[1;34m()\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m review:\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo review exist\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 15\u001b[0m REV \u001b[38;5;241m=\u001b[39m \u001b[43mreview\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m     16\u001b[0m REV_TAG \u001b[38;5;241m=\u001b[39m tagger(REV)  \n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m word, pos \u001b[38;5;129;01min\u001b[39;00m REV_TAG:\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "wReview()\n",
    "analyze()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
