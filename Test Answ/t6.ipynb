{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk, pandas as pd, pickle\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.stem import PorterStemmer,WordNetLemmatizer\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.classify import NaiveBayesClassifier,accuracy\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from string import punctuation\n",
    "from random import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "stems = PorterStemmer()\n",
    "lemms = WordNetLemmatizer()\n",
    "stopW = stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preproc(words):\n",
    "    tokens = word_tokenize(words)\n",
    "    words = [word.lower() for word in tokens if word.isalpha()] \n",
    "    words = [word for word in words if word not in stopW]\n",
    "    words = [stems.stem(word) for word in words]\n",
    "    words = [lemms.lemmatize(word) for word in words]\n",
    "    \n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = pd.read_csv(\"./dataset.csv\",encoding = \"utf-8-sig\")\n",
    "\n",
    "all_w = []\n",
    "\n",
    "for sent in ds[\"text\"]:\n",
    "    for word in preproc(sent):\n",
    "        all_w.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "fd = FreqDist(all_w)\n",
    "common = {word for word in fd.most_common(1000)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_set = []\n",
    "\n",
    "for txt,lbl in zip(ds[\"text\"],ds[\"label\"]):\n",
    "    feature = {}\n",
    "    \n",
    "    procd = preproc(txt)\n",
    "    \n",
    "    for word in procd:\n",
    "        feature[word] = (word in common)\n",
    "    \n",
    "    feature_set.append((feature,lbl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffle(feature_set)\n",
    "\n",
    "idx = int(len(feature_set)*0.7)\n",
    "trains = feature_set[:idx]\n",
    "tests = feature_set[idx:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "                 terribl = False          negati : positi =      9.6 : 1.0\n",
      "                    cold = False          negati : positi =      9.0 : 1.0\n",
      "                 horribl = False          negati : positi =      8.3 : 1.0\n",
      "                    okay = False          negati : positi =      8.3 : 1.0\n",
      "                 perfect = False          positi : negati =      7.8 : 1.0\n"
     ]
    }
   ],
   "source": [
    "clf = NaiveBayesClassifier.train(trains)\n",
    "clf.show_most_informative_features(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69.09 %\n"
     ]
    }
   ],
   "source": [
    "print(f\"{accuracy(clf,tests)*100:.2f} %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.69\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def synanto(words):\n",
    "    SYN = []\n",
    "    ANT = []\n",
    "    \n",
    "    for syns in wordnet.synsets(words):\n",
    "        for lemmas in syns.lemmas():\n",
    "            SYN.append(lemmas.name())\n",
    "            for ants in lemmas.antonyms():\n",
    "                ANT.append(ants.name())\n",
    "                \n",
    "    return SYN,ANT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "-\n",
      "==========\n",
      "['book', 'book', 'volume', 'record', 'record_book', 'book', 'script', 'book', 'playscript', 'ledger', 'leger', 'account_book', 'book_of_account', 'book', 'book', 'book', 'rule_book', 'Koran', 'Quran', \"al-Qur'an\", 'Book', 'Bible', 'Christian_Bible', 'Book', 'Good_Book', 'Holy_Scripture', 'Holy_Writ', 'Scripture', 'Word_of_God', 'Word', 'book', 'book', 'book', 'reserve', 'hold', 'book', 'book', 'book']\n",
      "['book', 'book', 'volume', 'record', 'record_book', 'book', 'script', 'book', 'playscript', 'ledger', 'leger', 'account_book', 'book_of_account', 'book', 'book', 'book', 'rule_book', 'Koran', 'Quran', \"al-Qur'an\", 'Book', 'Bible', 'Christian_Bible', 'Book', 'Good_Book', 'Holy_Scripture', 'Holy_Writ', 'Scripture', 'Word_of_God', 'Word', 'book', 'book', 'book', 'reserve', 'hold', 'book', 'book', 'book']\n",
      "==========\n",
      "['be', 'be', 'be', 'exist', 'be', 'be', 'equal', 'be', 'constitute', 'represent', 'make_up', 'comprise', 'be', 'be', 'follow', 'embody', 'be', 'personify', 'be', 'be', 'live', 'be', 'cost', 'be']\n",
      "['be', 'be', 'be', 'exist', 'be', 'be', 'equal', 'be', 'constitute', 'represent', 'make_up', 'comprise', 'be', 'be', 'follow', 'embody', 'be', 'personify', 'be', 'be', 'live', 'be', 'cost', 'be']\n",
      "==========\n",
      "['good', 'good', 'goodness', 'good', 'goodness', 'commodity', 'trade_good', 'good', 'good', 'full', 'good', 'good', 'estimable', 'good', 'honorable', 'respectable', 'beneficial', 'good', 'good', 'good', 'just', 'upright', 'adept', 'expert', 'good', 'practiced', 'proficient', 'skillful', 'skilful', 'good', 'dear', 'good', 'near', 'dependable', 'good', 'safe', 'secure', 'good', 'right', 'ripe', 'good', 'well', 'effective', 'good', 'in_effect', 'in_force', 'good', 'good', 'serious', 'good', 'sound', 'good', 'salutary', 'good', 'honest', 'good', 'undecomposed', 'unspoiled', 'unspoilt', 'good', 'well', 'good', 'thoroughly', 'soundly', 'good']\n",
      "['good', 'good', 'goodness', 'good', 'goodness', 'commodity', 'trade_good', 'good', 'good', 'full', 'good', 'good', 'estimable', 'good', 'honorable', 'respectable', 'beneficial', 'good', 'good', 'good', 'just', 'upright', 'adept', 'expert', 'good', 'practiced', 'proficient', 'skillful', 'skilful', 'good', 'dear', 'good', 'near', 'dependable', 'good', 'safe', 'secure', 'good', 'right', 'ripe', 'good', 'well', 'effective', 'good', 'in_effect', 'in_force', 'good', 'good', 'serious', 'good', 'sound', 'good', 'salutary', 'good', 'honest', 'good', 'undecomposed', 'unspoiled', 'unspoilt', 'good', 'well', 'good', 'thoroughly', 'soundly', 'good']\n",
      "==========\n"
     ]
    }
   ],
   "source": [
    "rev = \"the book is good\"\n",
    "\n",
    "tok_rev = word_tokenize(rev)\n",
    "\n",
    "for words in tok_rev:\n",
    "    syn,ant  = synanto(words)\n",
    "    print(syn if syn else \"-\")\n",
    "    print(syn if syn else \"-\")\n",
    "    print(\"=\"*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tagger(words):\n",
    "    tokens = word_tokenize(words)\n",
    "    tags = pos_tag(tokens)\n",
    "    return tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. the DT\n",
      "2. book NN\n",
      "3. is VBZ\n",
      "4. good JJ\n"
     ]
    }
   ],
   "source": [
    "tags = tagger(rev)\n",
    "\n",
    "for i,words in enumerate(tags):\n",
    "    print(f\"{i+1}. {words[0]} {words[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "negative\n"
     ]
    }
   ],
   "source": [
    "def analyze(words):\n",
    "    cleaned = preproc(words)\n",
    "    \n",
    "    res = clf.classify(FreqDist(cleaned))\n",
    "    print(res)\n",
    "\n",
    "analyze(\"blud\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
